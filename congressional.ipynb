{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/devkalavadiya/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/devkalavadiya/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "!pip install requests beautifulsoup4 pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape content from a given URL\n",
    "def scrape_content(url):\n",
    "    # Send request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parse the content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Main function to process the congressional record\n",
    "def main():\n",
    "    base_url = \"https://www.congress.gov\"\n",
    "    search_url = \"https://www.congress.gov/quick-search/congressional-record?wordsPhrases=&wordVariants=on&congressGroups[0]=0&congresses[0]=114&dateOperator=equal&startDate=&endDate=&dateIsOption=yesterday&sectionHouse=on&representative[0]=B000589&senator[0]=&pageSort=issueAsc\"\n",
    "    \n",
    "    # Initialize DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['Title', 'Date', 'Text', 'Remarks'])\n",
    "\n",
    "    # Scrape the search page\n",
    "    search_soup = scrape_content(search_url)\n",
    "\n",
    "    # Find all links to Congressional Record details from the search result\n",
    "    records = search_soup.find_all('a', class_='result-heading')\n",
    "    \n",
    "    for record in records:\n",
    "        record_href = record['href']\n",
    "        record_title = record.get_text(strip=True)\n",
    "        record_url = base_url + record_href\n",
    "        \n",
    "        # Scrape the individual record page\n",
    "        record_soup = scrape_content(record_url)\n",
    "\n",
    "        # Extract text and date from the page\n",
    "        date = record_soup.find('time').get_text(strip=True)\n",
    "        all_text = record_soup.find_all('pre')\n",
    "        text = '\\n'.join([text.get_text(strip=True) for text in all_text])\n",
    "        \n",
    "        # Assuming the Remarks are within a section with a specific id or class\n",
    "        remarks_section = record_soup.find('div', id='remarks') or record_soup.find('div', class_='remarks')\n",
    "        remarks = remarks_section.get_text(strip=True) if remarks_section else 'No remarks found'\n",
    "        \n",
    "        # Append to DataFrame\n",
    "        results_df = results_df.append({'Title': record_title, 'Date': date, 'Text': text, 'Remarks': remarks}, ignore_index=True)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('Congressional_Record_Boehner.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/devkalavadiya/miniconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.5.15\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/devkalavadiya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/devkalavadiya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/devkalavadiya/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/devkalavadiya/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/devkalavadiya/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/devkalavadiya/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/devkalavadiya/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/devkalavadiya/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "Prediction for new text: [-1]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Ensure nltk components are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess_text)\n",
    "\n",
    "# Example dataset\n",
    "texts = [\"I support this bill because it benefits our society.\", \n",
    "         \"This bill is harmful, and I oppose it.\",\n",
    "         \"I have no comments on this matter.\"]\n",
    "labels = [1, -1, 0]  # 1: support, -1: oppose, 0: neutral\n",
    "\n",
    "# Transform texts\n",
    "X = tfidf_vectorizer.fit_transform(texts)\n",
    "y = labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Classify new text\n",
    "# import text from sampletext.txt\n",
    "with open('sampletext.txt', 'r') as file:\n",
    "    new_text = file.read()\n",
    "new_text = \"I think this bill could be improved, but it's a good start.\"\n",
    "new_vector = tfidf_vectorizer.transform([new_text])\n",
    "prediction = model.predict(new_vector)\n",
    "print(f\"Prediction for new text: {prediction}\")\n",
    "\n",
    "#  Classify for Boeher\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      1.00      0.40         1\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.42      0.44      0.30         6\n",
      "weighted avg       0.54      0.33      0.32         6\n",
      "\n",
      "Prediction for new text: Against\n",
      "Extracted Pelosi Statements: [', our distinguished', '. I thank the gentleman for yielding and for his leadership', '', '']\n",
      "[', our distinguished', '. I thank the gentleman for yielding and for his leadership', '', '']\n",
      "Pelosi's statement: , our distinguished\n",
      "Pelosi's statement: . I thank the gentleman for yielding and for his leadership\n",
      "Pelosi's statement: \n",
      "Pelosi's statement: \n",
      "McCarthy's statement: ), the distinguished House\n",
      "McCarthy's statement: \n",
      "McCarthy's statement: \n",
      "McCarthy's statement: ), the distinguished House - Prediction: Against\n",
      "McCarthy's statement:  - Prediction: Against\n",
      "McCarthy's statement:  - Prediction: Against\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "\n",
    "\n",
    "# Sample data with possible sentiments and classification labels\n",
    "data = [\n",
    "    {'text': \"I support this bill because it benefits our society.\", 'label': 1},\n",
    "    {'text': \"This bill is harmful, and I oppose it.\", 'label': -1},\n",
    "    {'text': \"I have no comments on this matter.\", 'label': 0},\n",
    "    {'text': \"The provisions of this bill will advance our economic growth, which is why I fully back it.\", 'label': 1},\n",
    "    {'text': \"This legislation will negatively impact our education system, and I cannot support it.\", 'label': -1},\n",
    "    {'text': \"I am still reviewing the details of the bill and have not formed an opinion yet.\", 'label': 0},\n",
    "    {'text': \"I enthusiastically endorse this bill due to its environmental benefits.\", 'label': 1},\n",
    "    {'text': \"I must oppose this bill as it risks our national security.\", 'label': -1},\n",
    "    {'text': \"At this time, I choose to reserve my judgement until further information is available.\", 'label': 0},\n",
    "    {'text': \"After careful consideration, I believe this bill will significantly help our community, hence my support.\", 'label': 1},\n",
    "    {'text': \"I find the measures in this bill unacceptable and detrimental to our values, so I oppose it.\", 'label': -1},\n",
    "    {'text': \"I am undecided on this issue as more analysis is needed.\", 'label': 0},\n",
    "    {'text': \"This bill will strengthen our infrastructure, which is why I support it.\", 'label': 1},\n",
    "    {'text': \"Given the financial burden this bill imposes, I am against it.\", 'label': -1},\n",
    "    {'text': \"I have not yet decided where I stand on this legislation.\", 'label': 0},\n",
    "    {'text': \"I wholeheartedly support this bill for its positive impact on health care.\", 'label': 1},\n",
    "    {'text': \"The bill threatens to erode our civil liberties, and I cannot support it.\", 'label': -1},\n",
    "    {'text': \"I am currently abstaining from making a statement until further debate.\", 'label': 0},\n",
    "    {'text': \"This bill will create jobs, which is why it has my full support.\", 'label': 1},\n",
    "    {'text': \"I oppose this bill because it could potentially lead to environmental degradation.\", 'label': -1},\n",
    "    {'text': \"I am on the fence about this bill and will listen to my constituents further.\", 'label': 0}\n",
    "]\n",
    "\n",
    "\n",
    "# Splitting the dataset into texts and labels\n",
    "texts = [point['text'] for point in data]\n",
    "labels = [point['label'] for point in data]\n",
    "\n",
    "# Split data into training and test sets\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Creating a model pipeline with TF-IDF Vectorizer and Multinomial Naive Bayes Classifier\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Training the model\n",
    "model.fit(texts_train, labels_train)\n",
    "\n",
    "# Predicting the test set\n",
    "predicted_labels = model.predict(texts_test)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(labels_test, predicted_labels))\n",
    "\n",
    "# Function to classify new texts\n",
    "def predict_position(text):\n",
    "    prediction = model.predict([text])\n",
    "    return \"Support\" if prediction == 1 else \"Against\" if prediction == -1 else \"No participation\"\n",
    "\n",
    "with open('sampletext.txt', 'r') as file:\n",
    "    new_text = file.read()\n",
    "\n",
    "# Predicting the position for a new text\n",
    "prediction = predict_position(new_text)\n",
    "print(f\"Prediction for new text: {prediction}\")\n",
    "\n",
    "\n",
    "# Regular expression to find statements attributed to John Boehner\n",
    "boehner_statements = re.compile(r'(Speaker Boehner|John Boehner|Boehner):? (.*)')\n",
    "# Function to extract Boehner's statements from a given text file sampletext.txt\n",
    "def extract_boehner_statements(file_path):\n",
    "    boehner_statements = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            match = re.search(r'(Speaker Boehner|John Boehner|Boehner):? (.*)', line)\n",
    "            if match:\n",
    "                boehner_statements.append(match.group(2))\n",
    "    return boehner_statements\n",
    "\n",
    "# Extract Boehner's statements from the sample text file\n",
    "statements = extract_boehner_statements('sampletext.txt')\n",
    "\n",
    "# Print the extracted statements\n",
    "for statement in statements:\n",
    "    print(f\"Boehner's statement: {statement}\")\n",
    "\n",
    "# Function to classify Boehner's statements\n",
    "def classify_boehner_statements(statements):\n",
    "    classified_statements = []\n",
    "    for statement in statements:\n",
    "        prediction = predict_position(statement)\n",
    "        classified_statements.append((statement, prediction))\n",
    "    return classified_statements\n",
    "\n",
    "\n",
    "# Classify Boehner's statements\n",
    "classified_boehner_statements = classify_boehner_statements(statements)\n",
    "\n",
    "# Print the classified statements\n",
    "for statement, prediction in classified_boehner_statements:\n",
    "    print(f\"Boehner's statement: {statement} - Prediction: {prediction}\")\n",
    "\n",
    "\n",
    "def extract_pelosi_statements_advanced(text):\n",
    "    import re\n",
    "    pelosi_statements = []\n",
    "    in_pelosi_speech = False\n",
    "    current_statement = []\n",
    "\n",
    "    # Define regex for detecting any speaker or a new section in the text\n",
    "    speaker_regex = re.compile(r'\\sMr\\.\\s\\w+\\.\\sMr\\.\\sSpeaker', re.M)\n",
    "    pelosi_start_regex = re.compile(r'\\sMs\\.\\sPELOSI\\.', re.M)\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        # Check if a new speaker starts speaking\n",
    "        if pelosi_start_regex.search(line):\n",
    "            in_pelosi_speech = True\n",
    "            # Skip the speaker's name part and capture the statement\n",
    "            current_statement.append(line.split('.', 1)[-1].strip())\n",
    "            continue\n",
    "        \n",
    "        if in_pelosi_speech:\n",
    "            # If a new speaker section is detected and it's not Pelosi's continuation\n",
    "            if speaker_regex.search(line) and not pelosi_start_regex.search(line):\n",
    "                in_pelosi_speech = False\n",
    "                pelosi_statements.append(' '.join(current_statement).strip())\n",
    "                current_statement = []\n",
    "                continue\n",
    "            \n",
    "            # Continue capturing Pelosi's speech\n",
    "            current_statement.append(line.strip())\n",
    "\n",
    "    # Add the last captured statement if any\n",
    "    if in_pelosi_speech and current_statement:\n",
    "        pelosi_statements.append(' '.join(current_statement).strip())\n",
    "\n",
    "    return pelosi_statements\n",
    "\n",
    "# Example usage assuming your file is properly named and located\n",
    "pelosi_statements = extract_pelosi_statements('sampletext.txt')\n",
    "print(\"Extracted Pelosi Statements:\", pelosi_statements)\n",
    "\n",
    "\n",
    "# classify for Kevin McCarthy\n",
    "def extract_mccarthy_statements(file_path):\n",
    "    mccarthy_statements = []\n",
    "    pattern = re.compile(r'(Kevin McCarthy|McCarthy):?\\s*(.*)')\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # print(\"Processing line:\", line.strip())  # Debugging: show each line being processed\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                statement = match.group(2).strip()  # Capture the statement part and strip whitespace\n",
    "                mccarthy_statements.append(statement)\n",
    "                # print(\"Match found:\", statement)  # Debugging: show the match found\n",
    "            # else:\n",
    "                # print(\"No match found in line.\")  # Debugging: no match in this line\n",
    "    return mccarthy_statements\n",
    "\n",
    "# Example usage assuming your file is properly named and located\n",
    "mccarthy_statements = extract_mccarthy_statements('sampletext.txt')\n",
    "\n",
    "# Print the extracted statements\n",
    "for statement in mccarthy_statements:\n",
    "    print(f\"McCarthy's statement: {statement}\")\n",
    "\n",
    "# Function to classify McCarthy's statements\n",
    "def classify_mccarthy_statements(statements):\n",
    "    classified_statements = []\n",
    "    for statement in statements:\n",
    "        prediction = predict_position(statement)\n",
    "        classified_statements.append((statement, prediction))\n",
    "    return classified_statements\n",
    "\n",
    "# Classify McCarthy's statements\n",
    "classified_mccarthy_statements = classify_mccarthy_statements(mccarthy_statements)\n",
    "\n",
    "# Print the classified statements\n",
    "for statement, prediction in classified_mccarthy_statements:\n",
    "    print(f\"McCarthy's statement: {statement} - Prediction: {prediction}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
